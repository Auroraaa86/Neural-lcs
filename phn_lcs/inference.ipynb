{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### you can just run the code step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import PreTrainedTokenizerFast, T5EncoderModel, T5Config\n",
    "import os\n",
    "import json\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hard lcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lcs_new(seq1, seq2):\n",
    "        \"\"\"Find the longest common subsequence between two sequences using embedding distance.\"\"\"\n",
    "        ## seq1 is target\n",
    "        ## seq2 is source\n",
    "        \n",
    "        lengths = [[0] * (len(seq2) + 1) for _ in range(len(seq1) + 1)]\n",
    "        for i, x in enumerate(seq1):\n",
    "            for j, y in enumerate(seq2):\n",
    "                if  x == y:  # Adjust the threshold as needed\n",
    "                    ### Similar\n",
    "                    lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "                else:\n",
    "                    ### Not Similar\n",
    "                    lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "        \n",
    "        # Reconstruct the LCS\n",
    "        align_lcs_result = [] #tuple (src, target)\n",
    "        x, y = len(seq1), len(seq2)\n",
    "        \n",
    "        \n",
    "        while x != 0 and y != 0:\n",
    "\n",
    "            if lengths[x][y] == lengths[x-1][y]:\n",
    "                x -= 1\n",
    "            elif lengths[x][y] == lengths[x][y-1]:\n",
    "                align_lcs_result.append((seq2[y - 1], seq1[x - 1]))\n",
    "                y -= 1\n",
    "            else:\n",
    "                align_lcs_result.append((seq2[y - 1], seq1[x - 1]))\n",
    "                x -= 1\n",
    "                y -= 1\n",
    "                \n",
    "            if x == 0 and y != 0:\n",
    "                while y > 0: \n",
    "                    align_lcs_result.append((seq2[y - 1], seq1[0]))\n",
    "                    y -= 1\n",
    "                break\n",
    "            if x != 0 and y == 0:\n",
    "                align_lcs_result.append((seq2[0], seq1[x - 1]))\n",
    "                break\n",
    "        \n",
    "        align_lcs_result.reverse()\n",
    "\n",
    "        return align_lcs_result\n",
    "\n",
    "def insert_missing_tuples(ref_words, align_result):\n",
    "    # Extract the second elements from the align_result tuples\n",
    "    align_words = []\n",
    "    for t in align_result:\n",
    "        if len(align_words) == 0:\n",
    "            align_words.append(t[1])\n",
    "        elif align_words[-1] != t[1]:\n",
    "            align_words.append(t[1])\n",
    "            \n",
    "    missing_flg = [1] * len(ref_words)\n",
    "\n",
    "    for i, word in enumerate(ref_words):\n",
    "        missing_num = sum(missing_flg[:i])\n",
    "        j = i - missing_num\n",
    "        if j <len(align_words) and align_words[j] == word:\n",
    "            missing_flg[i] = 0\n",
    "\n",
    "    # Create a copy of align_result to insert missing words\n",
    "    new_align_result = []\n",
    "    ref_idx = 0  # Index to track the current position in ref_words\n",
    "\n",
    "    for idx, word in enumerate(ref_words):\n",
    "        # Insert missing words\n",
    "        if missing_flg[idx] == 1:\n",
    "            new_align_result.append((None, word))\n",
    "        # Add the next word from align_result if it matches\n",
    "        while ref_idx < len(align_result) and align_result[ref_idx][1] == word:\n",
    "            new_align_result.append(align_result[ref_idx])\n",
    "            ref_idx += 1\n",
    "\n",
    "    return new_align_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMU pho2dict\n",
    "phoneme_to_id = {\n",
    "    \"AA\": 0, \"AE\": 1, \"AH\": 2, \"AO\": 3, \"AW\": 4, \"AY\": 5,\n",
    "    \"B\": 6, \"CH\": 7, \"D\": 8, \"DH\": 9, \"EH\": 10, \"ER\": 11,\n",
    "    \"EY\": 12, \"F\": 13, \"G\": 14, \"HH\": 15, \"IH\": 16, \"IY\": 17,\n",
    "    \"JH\": 18, \"K\": 19, \"L\": 20, \"M\": 21, \"N\": 22, \"NG\": 23,\n",
    "    \"OW\": 24, \"OY\": 25, \"P\": 26, \"R\": 27, \"S\": 28, \"SH\": 29,\n",
    "    \"T\": 30, \"TH\": 31, \"UH\": 32, \"UW\": 33, \"V\": 34, \"W\": 35,\n",
    "    \"Y\": 36, \"Z\": 37, \"ZH\": 38,\n",
    "    \"<pad>\": 39, \"<unk>\": 40, \"<cls>\": 41, \"<sep>\": 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customed tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zongli/anaconda3/envs/speech/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class PhonemeTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, phoneme_to_id, **kwargs):\n",
    "        self.phoneme_to_id = phoneme_to_id\n",
    "        super().__init__(**kwargs)\n",
    "        self.id_to_phoneme = {v: k for k, v in phoneme_to_id.items()}\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.cls_token = \"<cls>\"\n",
    "        self.sep_token = \"<sep>\"\n",
    "        \n",
    "    def get_vocab(self):\n",
    "        # return vocabulary table\n",
    "        return self.phoneme_to_id\n",
    "        \n",
    "    def _convert_token_to_id(self, token):\n",
    "        # Convert a single phoneme to its ID\n",
    "        return self.phoneme_to_id.get(token, self.phoneme_to_id.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        # Convert a single ID back to a phoneme\n",
    "        return self.id_to_phoneme.get(index, self.unk_token)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        # Split text into phonemes and map them to IDs\n",
    "        return [self.phoneme_to_id.get(phoneme, self.phoneme_to_id.get(self.unk_token)) for phoneme in text.split()]\n",
    "\n",
    "    def encode(self, text, max_length = 120, add_special_tokens = True, padding = True):\n",
    "        max_len = max_length\n",
    "        token_ids = self._tokenize(text)\n",
    "        if add_special_tokens:\n",
    "            token_ids = token_ids + [self.phoneme_to_id[self.sep_token]]\n",
    "        if padding:\n",
    "            prev_len = len(token_ids)\n",
    "            token_ids = token_ids + [self.phoneme_to_id[self.pad_token]] * (max_len - prev_len)\n",
    "            mask = [1] * prev_len + [0] * (max_len - prev_len)\n",
    "        if padding:\n",
    "            return {\"input_ids\": torch.tensor(token_ids), \"attention_mask\": torch.tensor(mask)}\n",
    "        else:\n",
    "            return {\"input_ids\": torch.tensor(token_ids), \"attention_mask\": None}\n",
    "                \n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        tokens = [self.id_to_phoneme[token_id] for token_id in token_ids if token_id in self.id_to_phoneme]\n",
    "        if skip_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in [self.pad_token, self.cls_token, self.sep_token]]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.phoneme_to_id)\n",
    "\n",
    "tokenizer = PhonemeTokenizer(phoneme_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### structure of Phoneme soft aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhonemeBoundaryAlignerT5(nn.Module):\n",
    "    def __init__(self, pretrained_model_name=\"t5-small\", phoneme_vocab_size=50, hidden_dim=512, num_filters = 16):\n",
    "        super(PhonemeBoundaryAlignerT5, self).__init__()\n",
    "        # Load pretrained T5 model\n",
    "        self.encoder = T5EncoderModel.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        # Resize token embeddings to fit phoneme vocab size\n",
    "        self.encoder.resize_token_embeddings(phoneme_vocab_size)\n",
    "\n",
    "        # 1D CNN Layer\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels = hidden_dim * 2,\n",
    "            out_channels = num_filters,\n",
    "            kernel_size = 3,\n",
    "            stride = 1,\n",
    "            padding = 1\n",
    "        )\n",
    "                \n",
    "        # Boundary Predictor: Fully connected layers\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_filters, num_filters//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_filters//2, 4),\n",
    "        )\n",
    "        \n",
    "        # Re-initialize weights if needed\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, ref, src, ref_mask=None, src_mask=None):\n",
    "        # Encode reference and source phonemes using T5\n",
    "        ref_output = self.encoder(input_ids=ref, attention_mask=ref_mask).last_hidden_state  # (batch_size, src_len, hidden_dim)\n",
    "        src_output = self.encoder(input_ids=src, attention_mask=src_mask).last_hidden_state  # (batch_size, src_len, hidden_dim)\n",
    "        \n",
    "        # Combine ref and src features\n",
    "        alignment_features = torch.cat((ref_output, src_output), dim=-1)  # (batch_size, src_len, hidden_dim * 2)\n",
    "        alignment_features = alignment_features.permute(0, 2, 1)                               # (batch_size, hidden_dim * 2, src_len)\n",
    "        \n",
    "        # print(alignment_features.shape)\n",
    "        \n",
    "        # Process with Conv layer to capture contextual information\n",
    "        conv_features = self.conv1d(alignment_features)     # (batch_size, num_filters, src_len) \n",
    "        conv_features = conv_features.permute(0, 2, 1)                      # (batch_size, src_len, num_filters)\n",
    "        \n",
    "        # Through MLP layer\n",
    "        mlp_features = self.MLP(conv_features) # (batch_size, src_len, 3)\n",
    "        \n",
    "        return mlp_features\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Reinitialize model parameters for layers other than embeddings.\n",
    "        \"\"\"\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, torch.nn.Conv1d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, torch.nn.Embedding):\n",
    "                torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        # Apply custom weight initialization to layers\n",
    "        self.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune Neural lcs with hard lcs to enhance robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(align_result, ref, src):\n",
    "    if align_result[-1][1] == ref[-1] and align_result[-1][0] == src[-1]: return align_result\n",
    "    elif align_result[-1][1] == align_result[-1][0]: return align_result\n",
    "    else:\n",
    "        mark = 0\n",
    "        split_bd = None\n",
    "        split_sign = None\n",
    "        idx = len(align_result) - 1\n",
    "        while idx > 0:\n",
    "            if align_result[idx][0] == align_result[idx][1]: \n",
    "                mark += 1\n",
    "            if mark == 2:\n",
    "                j = idx - 1\n",
    "                while j >= 0 and align_result[j][1] == align_result[idx][1]:\n",
    "                    j -= 1\n",
    "                split_bd = align_result[j][1]\n",
    "                split_sign = j\n",
    "                break\n",
    "            idx -= 1 \n",
    "        # print(split_bd)\n",
    "        if mark != 2 or split_bd == None: return align_result\n",
    "\n",
    "        align_right = deepcopy(align_result)\n",
    "        \n",
    "        idx = len(align_result) - 1\n",
    "        new_src = []\n",
    "        new_ref = []\n",
    "        while idx >= 0 and idx != split_sign:\n",
    "            align_right.pop()\n",
    "            if align_result[idx][0] != None:\n",
    "                new_src.insert(0, align_result[idx][0])\n",
    "            idx -= 1\n",
    "        \n",
    "        idx = len(align_result) - 1\n",
    "        while idx > 0 and idx != split_sign:\n",
    "            if align_result[idx][1] == None:\n",
    "                idx -= 1\n",
    "                continue\n",
    "            jdx = idx\n",
    "            while jdx >= 0 and align_result[jdx][1] == align_result[idx][1]:\n",
    "                jdx -= 1\n",
    "            jdx += 1\n",
    "            idx = jdx\n",
    "            new_ref.insert(0, align_result[jdx][1])\n",
    "            idx -= 1\n",
    "\n",
    "        # print(new_ref, new_src)\n",
    "\n",
    "        new_result = align_right + insert_missing_tuples(new_ref, find_lcs_new(new_ref, new_src))\n",
    "        return new_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3415191/2321413034.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location = device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('EH', 'EH'), ('V', 'V'), ('R', 'R'), ('IY', 'IY'), ('AE', 'IY'), (None, 'W'), (None, 'AH'), ('N', 'N'), ('IH', 'IH'), ('Z', 'Z'), ('T', 'T'), ('UW', 'UW'), ('AH', 'AH'), ('P', 'P'), ('S', 'S'), ('EH', 'EH'), ('D', 'EH'), ('T', 'T'), ('UW', 'UW'), ('K', 'K'), ('AA', 'AA'), ('M', 'M'), ('EH', 'EH'), ('N', 'N'), ('T', 'T')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3415191/2321413034.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ref = torch.tensor(ref).unsqueeze(0).to(device)\n",
      "/tmp/ipykernel_3415191/2321413034.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src = torch.tensor(src).unsqueeze(0).to(device)\n",
      "/tmp/ipykernel_3415191/2321413034.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ref_mask = torch.tensor(ref_mask).unsqueeze(0).to(device)\n",
      "/tmp/ipykernel_3415191/2321413034.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src_mask = torch.tensor(src_mask).unsqueeze(0).to(device)\n"
     ]
    }
   ],
   "source": [
    "model_path = \"model/phn_align_2.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PhonemeBoundaryAlignerT5(phoneme_vocab_size=len(tokenizer))\n",
    "model.load_state_dict(torch.load(model_path, map_location = device))\n",
    "model.to(device)\n",
    "\n",
    "def neuralLCS(seq1, seq2 ,model):\n",
    "    ## seq1 is target\n",
    "    ## seq2 is source\n",
    "    tokenizer = PhonemeTokenizer(phoneme_to_id)\n",
    "    ref = tokenizer.encode(\" \".join(seq1), max_length = 120)[\"input_ids\"]\n",
    "    src = tokenizer.encode(\" \".join(seq2), max_length = 120)[\"input_ids\"]\n",
    "    ref_mask = tokenizer.encode(\" \".join(seq1), max_length = 120)[\"attention_mask\"]\n",
    "    src_mask = tokenizer.encode(\" \".join(seq2), max_length = 120)[\"attention_mask\"]\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ref = torch.tensor(ref).unsqueeze(0).to(device)\n",
    "    src = torch.tensor(src).unsqueeze(0).to(device)\n",
    "    ref_mask = torch.tensor(ref_mask).unsqueeze(0).to(device)\n",
    "    src_mask = torch.tensor(src_mask).unsqueeze(0).to(device)\n",
    "\n",
    "    output = model(ref, src, ref_mask, src_mask).squeeze(0)\n",
    "    predicted_classes = torch.argmax(output, dim=1).tolist()\n",
    "\n",
    "    prediction = predicted_classes[: predicted_classes.index(3) + 1]\n",
    "    # prediction = prediction[: -1]\n",
    "    # print(prediction)\n",
    "\n",
    "    aln_mark = 0\n",
    "    src_mark = 0\n",
    "    align_result = []\n",
    "    left = None\n",
    "    for jdx, target in enumerate(seq1):\n",
    "        if prediction[aln_mark] == 3: \n",
    "            left = list(range(jdx, len(seq1)))\n",
    "            break\n",
    "        elif prediction[aln_mark] == 2: \n",
    "            align_result.append((None, target))\n",
    "            aln_mark += 1\n",
    "            continue\n",
    "        elif prediction[aln_mark] == 0:\n",
    "            i = aln_mark\n",
    "            while True:\n",
    "                if prediction[i] == 0 and src_mark < len(seq2): \n",
    "                    align_result.append((seq2[src_mark], target))\n",
    "                    i += 1\n",
    "                    src_mark += 1\n",
    "                else: break\n",
    "            aln_mark = i\n",
    "        \n",
    "        if prediction[aln_mark] == 1:\n",
    "            if src_mark < len(seq2):\n",
    "                align_result.append((seq2[src_mark], target))\n",
    "                aln_mark += 1\n",
    "                src_mark += 1\n",
    "\n",
    "    if left == None and jdx < len(seq1) - 1:\n",
    "        left = list(range(jdx, len(seq1)))\n",
    "\n",
    "    if left != None:\n",
    "        for item in left: align_result.append((None, seq1[item]))\n",
    "\n",
    "    if src_mark - 1 < len(seq2) - 1:\n",
    "        for item in list(range(src_mark, len(seq2))): align_result.append((seq2[src_mark], None))\n",
    "\n",
    "    return correct(align_result, seq1, seq2)\n",
    "\n",
    "seq1 = 'EH V R IY W AH N IH Z T UW AH P S EH T UW K AA M EH N T'.split(\" \")\n",
    "seq2 = 'EH V R IY AE N IH Z T UW AH P S EH D T UW K AA M EH N T'.split(\" \")\n",
    "\n",
    "align_nn_lcs = neuralLCS(seq1, seq2, model)\n",
    "print(align_nn_lcs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

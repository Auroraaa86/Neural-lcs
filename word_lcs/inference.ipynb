{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### you can just run the code step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import PreTrainedTokenizerFast, T5EncoderModel, T5Config, T5Tokenizer\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hard lcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lcs_new(seq1, seq2):\n",
    "        \"\"\"Find the longest common subsequence between two sequences using embedding distance.\"\"\"\n",
    "        ## seq1 is target\n",
    "        ## seq2 is source\n",
    "        \n",
    "        lengths = [[0] * (len(seq2) + 1) for _ in range(len(seq1) + 1)]\n",
    "        for i, x in enumerate(seq1):\n",
    "            for j, y in enumerate(seq2):\n",
    "                if  x == y:  # Adjust the threshold as needed\n",
    "                    ### Similar\n",
    "                    lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "                else:\n",
    "                    ### Not Similar\n",
    "                    lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "        \n",
    "        # Reconstruct the LCS\n",
    "        align_lcs_result = [] #tuple (src, target)\n",
    "        x, y = len(seq1), len(seq2)\n",
    "        \n",
    "        \n",
    "        while x != 0 and y != 0:\n",
    "\n",
    "            if lengths[x][y] == lengths[x-1][y]:\n",
    "                x -= 1\n",
    "            elif lengths[x][y] == lengths[x][y-1]:\n",
    "                align_lcs_result.append((seq2[y - 1], seq1[x - 1]))\n",
    "                y -= 1\n",
    "            else:\n",
    "                align_lcs_result.append((seq2[y - 1], seq1[x - 1]))\n",
    "                x -= 1\n",
    "                y -= 1\n",
    "                \n",
    "            if x == 0 and y != 0:\n",
    "                while y > 0: \n",
    "                    align_lcs_result.append((seq2[y - 1], seq1[0]))\n",
    "                    y -= 1\n",
    "                break\n",
    "            if x != 0 and y == 0:\n",
    "                align_lcs_result.append((seq2[0], seq1[x - 1]))\n",
    "                break\n",
    "        \n",
    "        align_lcs_result.reverse()\n",
    "\n",
    "        return align_lcs_result\n",
    "\n",
    "def insert_missing_tuples(ref_words, align_result):\n",
    "    # Extract the second elements from the align_result tuples\n",
    "    align_words = []\n",
    "    for t in align_result:\n",
    "        if len(align_words) == 0:\n",
    "            align_words.append(t[1])\n",
    "        elif align_words[-1] != t[1]:\n",
    "            align_words.append(t[1])\n",
    "            \n",
    "    missing_flg = [1] * len(ref_words)\n",
    "\n",
    "    for i, word in enumerate(ref_words):\n",
    "        missing_num = sum(missing_flg[:i])\n",
    "        j = i - missing_num\n",
    "        if j <len(align_words) and align_words[j] == word:\n",
    "            missing_flg[i] = 0\n",
    "\n",
    "    # Create a copy of align_result to insert missing words\n",
    "    new_align_result = []\n",
    "    ref_idx = 0  # Index to track the current position in ref_words\n",
    "\n",
    "    for idx, word in enumerate(ref_words):\n",
    "        # Insert missing words\n",
    "        if missing_flg[idx] == 1:\n",
    "            new_align_result.append((None, word))\n",
    "        # Add the next word from align_result if it matches\n",
    "        while ref_idx < len(align_result) and align_result[ref_idx][1] == word:\n",
    "            new_align_result.append(align_result[ref_idx])\n",
    "            ref_idx += 1\n",
    "\n",
    "    return new_align_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### structure of Phoneme soft aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordBoundaryAlignerT5(nn.Module):\n",
    "    def __init__(self, pretrained_model_name=\"t5-small\", gru_hidden_dim = 128, num_filters = 16, hidden_dim=512):\n",
    "        super(WordBoundaryAlignerT5, self).__init__()\n",
    "        # Load pretrained T5 model\n",
    "        self.encoder = T5EncoderModel.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        # GRU Layer to capture long sentence feature\n",
    "        self.gru = nn.GRU(input_size = hidden_dim * 2, hidden_size = gru_hidden_dim, \n",
    "                          num_layers = 2, batch_first = True, bidirectional = True)\n",
    "\n",
    "        # 1D CNN Layer\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels = gru_hidden_dim * 2,\n",
    "            out_channels = num_filters,\n",
    "            kernel_size = 3,\n",
    "            stride = 1,\n",
    "            padding = 1\n",
    "        )\n",
    "                \n",
    "        # Boundary Predictor: Fully connected layers\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_filters, num_filters//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_filters//2, 4),\n",
    "        )\n",
    "        \n",
    "        # Re-initialize weights if needed\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, ref, src, ref_mask=None, src_mask=None):\n",
    "        # Encode reference and source phonemes using T5\n",
    "        ref_output = self.encoder(input_ids=ref, attention_mask=ref_mask).last_hidden_state  # (batch_size, src_len, hidden_dim)\n",
    "        src_output = self.encoder(input_ids=src, attention_mask=src_mask).last_hidden_state  # (batch_size, src_len, hidden_dim)\n",
    "        \n",
    "        # Combine ref and src features\n",
    "        alignment_features = torch.cat((ref_output, src_output), dim=-1)  # (batch_size, src_len, hidden_dim * 2)\n",
    "        alignment_features, _ = self.gru(alignment_features)              # (batch_size, src_len, gru_hidden_dim * 2)\n",
    "\n",
    "        alignment_features = alignment_features.permute(0, 2, 1)          # (batch_size, gru_hidden_dim * 2, src_len)\n",
    "        \n",
    "        # Process with Conv layer to capture contextual information\n",
    "        conv_features = self.conv1d(alignment_features)     # (batch_size, num_filters, src_len) \n",
    "        conv_features = conv_features.permute(0, 2, 1)      # (batch_size, src_len, num_filters)\n",
    "        \n",
    "        # Through MLP layer\n",
    "        mlp_features = self.MLP(conv_features) # (batch_size, src_len, 3)\n",
    "        \n",
    "        return mlp_features\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Reinitialize model parameters for layers other than embeddings.\n",
    "        \"\"\"\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, torch.nn.Conv1d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, torch.nn.Embedding):\n",
    "                torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        # Apply custom weight initialization to layers\n",
    "        self.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune Neural lcs with hard lcs to enhance robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(align_result, ref, src):\n",
    "    if align_result[-1][1] == ref[-1] and align_result[-1][0] == src[-1]: return align_result\n",
    "    elif align_result[-1][1] == align_result[-1][0]: return align_result\n",
    "    else:\n",
    "        mark = 0\n",
    "        split_bd = None\n",
    "        split_sign = None\n",
    "        idx = len(align_result) - 1\n",
    "        while idx > 0:\n",
    "            if align_result[idx][0] == align_result[idx][1]: \n",
    "                mark += 1\n",
    "            if mark == 2:\n",
    "                j = idx - 1\n",
    "                while j >= 0 and align_result[j][1] == align_result[idx][1]:\n",
    "                    j -= 1\n",
    "                split_bd = align_result[j][1]\n",
    "                split_sign = j\n",
    "                break\n",
    "            idx -= 1 \n",
    "        # print(split_bd)\n",
    "        if mark != 2 or split_bd == None: return align_result\n",
    "\n",
    "        align_right = deepcopy(align_result)\n",
    "        \n",
    "        idx = len(align_result) - 1\n",
    "        new_src = []\n",
    "        new_ref = []\n",
    "        while idx >= 0 and idx != split_sign:\n",
    "            align_right.pop()\n",
    "            if align_result[idx][0] != None:\n",
    "                new_src.insert(0, align_result[idx][0])\n",
    "            idx -= 1\n",
    "        \n",
    "        idx = len(align_result) - 1\n",
    "        while idx > 0 and idx != split_sign:\n",
    "            if align_result[idx][1] == None:\n",
    "                idx -= 1\n",
    "                continue\n",
    "            jdx = idx\n",
    "            while jdx >= 0 and align_result[jdx][1] == align_result[idx][1]:\n",
    "                jdx -= 1\n",
    "            jdx += 1\n",
    "            idx = jdx\n",
    "            new_ref.insert(0, align_result[jdx][1])\n",
    "            idx -= 1\n",
    "\n",
    "        # print(new_ref, new_src)\n",
    "\n",
    "        new_result = align_right + insert_missing_tuples(new_ref, find_lcs_new(new_ref, new_src))\n",
    "        return new_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('You', 'You'), ('wish', 'wish'), ('to', 'to'), ('to', 'to'), (None, 'know'), ('know', 'all'), ('all', 'about'), ('about', 'my'), ('me', 'grandfather'), ('grandfather', None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3452526/2908490012.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location = device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('You', 'You'),\n",
       " ('wish', 'wish'),\n",
       " ('to', 'to'),\n",
       " ('to', 'to'),\n",
       " ('know', 'know'),\n",
       " ('all', 'all'),\n",
       " ('about', 'about'),\n",
       " ('me', 'about'),\n",
       " (None, 'my'),\n",
       " ('grandfather', 'grandfather')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"model/word_align_1.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "model = WordBoundaryAlignerT5()\n",
    "model.load_state_dict(torch.load(model_path, map_location = device))\n",
    "model.to(device)\n",
    "\n",
    "def neuralLCS(seq1, seq2 ,model, tokenizer):\n",
    "    ## seq1 is target\n",
    "    ## seq2 is source\n",
    "    ref = tokenizer(\" \".join(seq1), max_length = 50 ,padding = \"max_length\", return_tensors = \"pt\")[\"input_ids\"].to(device)\n",
    "    src = tokenizer(\" \".join(seq2), max_length = 50 ,padding = \"max_length\", return_tensors = \"pt\")[\"input_ids\"].to(device)\n",
    "    ref_mask = tokenizer(\" \".join(seq1), max_length = 50 ,padding = \"max_length\", return_tensors = \"pt\")[\"attention_mask\"].to(device)\n",
    "    src_mask = tokenizer(\" \".join(seq2), max_length = 50 ,padding = \"max_length\", return_tensors = \"pt\")[\"attention_mask\"].to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    output = model(ref, src, ref_mask, src_mask).squeeze(0)\n",
    "    predicted_classes = torch.argmax(output, dim=1).tolist()\n",
    "\n",
    "    prediction = predicted_classes[: predicted_classes.index(3) + 1]\n",
    "\n",
    "    aln_mark = 0\n",
    "    src_mark = 0\n",
    "    align_result = []\n",
    "    left = None\n",
    "    for jdx, target in enumerate(seq1):\n",
    "        if prediction[aln_mark] == 3: \n",
    "            left = list(range(jdx, len(seq1)))\n",
    "            break\n",
    "        elif prediction[aln_mark] == 2: \n",
    "            align_result.append((None, target))\n",
    "            aln_mark += 1\n",
    "            continue\n",
    "        elif prediction[aln_mark] == 0:\n",
    "            i = aln_mark\n",
    "            while True:\n",
    "                if prediction[i] == 0 and src_mark < len(seq2): \n",
    "                    align_result.append((seq2[src_mark], target))\n",
    "                    i += 1\n",
    "                    src_mark += 1\n",
    "                else: break\n",
    "            aln_mark = i\n",
    "        \n",
    "        if prediction[aln_mark] == 1:\n",
    "            if src_mark < len(seq2):\n",
    "                align_result.append((seq2[src_mark], target))\n",
    "                aln_mark += 1\n",
    "                src_mark += 1\n",
    "\n",
    "    if left == None and jdx < len(seq1) - 1:\n",
    "        left = list(range(jdx, len(seq1)))\n",
    "\n",
    "    if left != None:\n",
    "        for item in left: align_result.append((None, seq1[item]))\n",
    "\n",
    "    if src_mark - 1 < len(seq2) - 1:\n",
    "        for item in list(range(src_mark, len(seq2))): align_result.append((seq2[src_mark], None))\n",
    "\n",
    "    return correct(align_result, seq1, seq2)\n",
    "\n",
    "seq1 = \"You wish to know all about my grandfather\".split(\" \")\n",
    "seq2 = \"You wish to to know all about me grandfather\".split(\" \")\n",
    "\n",
    "align_nn_lcs = neuralLCS(seq1, seq2, model, tokenizer)\n",
    "align_nn_lcs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
